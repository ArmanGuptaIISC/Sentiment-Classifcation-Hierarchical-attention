{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data1/home/armangupta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data1/home/armangupta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import nltk\n",
    "from  collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import contractions\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/data1/home/armangupta/glove.6B/glove.6B.300d.txt'\n",
    "output_file = '/data1/home/armangupta/Semester 3/DLNLP/Assignment3/gensim_glove.6B.300d.txt'\n",
    "EMB_SIZE  = 300\n",
    "gensim_model = KeyedVectors.load_word2vec_format(output_file,binary= False)\n",
    "word2int = gensim_model.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfilepath = 'data/Imdb/Train.csv'\n",
    "train = pd.read_csv(trainfilepath)\n",
    "train_reviews = train['review'].to_numpy()\n",
    "label = train['sentiment'].apply(lambda x : 1 if x=='positive' else 0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000,) (4000,) (36000,) (4000,)\n"
     ]
    }
   ],
   "source": [
    "train_reviews , val_reviews , label , val_label = train_test_split(train_reviews , label, test_size = 0.1)\n",
    "print(train_reviews.shape , val_reviews.shape, label.shape, val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/data1/home/armangupta/Semester 3/DLNLP/Assignment 2/E0334 Assignment2 Test Dataset.csv', names = ['review','sentiment'])\n",
    "test_reviews = test['review'].to_numpy()\n",
    "test_label = test['sentiment'].apply(lambda x : 1 if x=='positive' else 0).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def rm_link(self,text):\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "    def rm_html(self,text):\n",
    "        return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    def space_bt_punct(self,text):\n",
    "        pattern = r'([.,!?-])'\n",
    "        s = re.sub(pattern, r'\\1 ', text)     # add whitespaces between punctuation\n",
    "        s = re.sub(r'\\s{2,}', ' ', s)        # remove double whitespaces    \n",
    "        return s\n",
    "    def space_bt_punct2(self,text):\n",
    "        pattern = r'([.,!?-])'\n",
    "        s = re.sub(pattern, r' \\1 ', text)     # add whitespaces between punctuation\n",
    "        s = re.sub(r'\\s{2,}', ' ', s)        # remove double whitespaces    \n",
    "        return s\n",
    "        \n",
    "    def rm_punct2(self,text):\n",
    "        return re.sub(r'[\\\"\\#\\$\\%\\&\\-\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~\\.\\,\\!\\?]', ' ', text)\n",
    "\n",
    "    def rm_number(self,text):\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "\n",
    "    def rm_whitespaces(self,text):\n",
    "        return re.sub(r' +', ' ', text)\n",
    "\n",
    "    def rm_nonascii(self,text):\n",
    "        return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "    def rm_emoji(self,text):\n",
    "        emojis = re.compile(\n",
    "            '['\n",
    "            u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "            u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "            u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "            u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "            u'\\U00002702-\\U000027B0'\n",
    "            u'\\U000024C2-\\U0001F251'\n",
    "            ']+',\n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "        return emojis.sub(r'', text)\n",
    "\n",
    "    def spell_correction(self,text):\n",
    "        return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "    def contraction(self,text):\n",
    "        expanded_words = []   \n",
    "        for word in text.split():\n",
    "            # using contractions.fix to expand the shortened words\n",
    "            expanded_words.append(contractions.fix(word))  \n",
    "        return ' '.join(expanded_words)\n",
    "\n",
    "    def clean_pipeline_phase1(self,text):\n",
    "        contract_text = self.contraction(text)\n",
    "        spell_corrected = self.spell_correction(contract_text)   \n",
    "        no_link = self.rm_link(contract_text)\n",
    "        no_html = self.rm_html(no_link)\n",
    "        space_punct = self.space_bt_punct(no_html)\n",
    "        return space_punct\n",
    "\n",
    "    def clean_pipeline_phase2(self,text):\n",
    "        space_punct = self.space_bt_punct2(text)\n",
    "        no_punct = self.rm_punct2(space_punct)\n",
    "        no_number = self.rm_number(no_punct)\n",
    "        no_whitespaces = self.rm_whitespaces(no_number)\n",
    "        no_nonasci = self.rm_nonascii(no_whitespaces)\n",
    "        no_emoji = self.rm_emoji(no_nonasci)\n",
    "        spell_corrected = self.spell_correction(no_emoji)\n",
    "        return spell_corrected.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = CleanText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_text_reviews = [nltk.sent_tokenize(clean.clean_pipeline_phase1(text)) for text in train_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text_reviews = []\n",
    "words = []\n",
    "for document in document_text_reviews:\n",
    "    tmp = []\n",
    "    for sent in document:\n",
    "        t = clean.clean_pipeline_phase2(sent)\n",
    "        t = t.lower()\n",
    "        if t != '':\n",
    "            tmp.append(t)\n",
    "            words.extend(word_tokenize(t))\n",
    "    sent_text_reviews.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_len = []\n",
    "for document in document_text_reviews:\n",
    "    temp_len.append(len(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_80 = (int)(np.quantile(temp_len, 0.8))\n",
    "q_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this movie pretty much sucked',\n",
       " 'it will be a better movie also if your not in the military',\n",
       " 'it will be a better movie also if your not in the military']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(sent_text_reviews[0] , k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sent_text_reviews , opt_label = [],[]\n",
    "for i in range(len(label)):\n",
    "    document,l = sent_text_reviews[i] , label[i]\n",
    "    if len(document) > q_80:\n",
    "        times =(int)( np.round_(len(document)/q_80))\n",
    "        for j in range(times):\n",
    "            opt_sent_text_reviews.append(random.sample(document , k= q_80))\n",
    "            opt_label.append(l)\n",
    "    else:\n",
    "        opt_sent_text_reviews.append(document)\n",
    "        opt_label.append(l)\n",
    "assert len(opt_sent_text_reviews) == len(opt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18., 18., 18., 18., 18.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_len2 = []\n",
    "for document in opt_sent_text_reviews:\n",
    "    temp_len2.append(len(document))\n",
    "np.quantile(temp_len2,[0.8,.9,.95,.99,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Intergrating the vocabulary from the Given Data\n",
    "# corpus = ' '.join(train_reviews)\n",
    "# words = corpus.split()\n",
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "remain_train_words = set(vocab).difference(word2int.keys())\n",
    "random_vector_initialisation = np.random.normal(0,3 , size = (len(remain_train_words) , EMB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = max(list(word2int.values())) + 1\n",
    "remain_train_word_w2i = dict(zip(remain_train_words,np.arange(start , start+len(remain_train_words))))\n",
    "word2int.update(remain_train_word_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int['the'] = max(list(word2int.values())) + 1\n",
    "word2int['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = gensim_model.vectors\n",
    "embeddings = np.vstack((embedding_vectors,random_vector_initialisation))\n",
    "embeddings = np.vstack((np.zeros((1,EMB_SIZE)) ,embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_train_encoder = [[[word2int[w] for w in  sent.split()] for sent in review]  for review in opt_sent_text_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sent = 0\n",
    "max_len_doc = 0\n",
    "for review in sent_train_encoder:\n",
    "    for sent in review:\n",
    "        max_len_sent = max(len(sent),max_len_sent)\n",
    "    max_len_doc = max(max_len_doc , len(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 350)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_doc , max_len_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_sentence(review, pad_len_document =18 ,pad_len_sent = 50 ,pad_id = 0):\n",
    "    features = np.full((pad_len_document, pad_len_sent) , pad_id , dtype=int )\n",
    "    for i, sent in enumerate(review):\n",
    "        if i < pad_len_document:\n",
    "            features[i, :len(sent)] = np.array(sent)[:pad_len_sent]\n",
    "    return features\n",
    "\n",
    "train_features = np.array([padding_sentence(review) for review in sent_train_encoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 14, 28, ...,  0,  0,  0],\n",
       "       [29, 22, 10, ...,  0,  0,  0],\n",
       "       [ 4, 17, 11, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [39, 29, 14, ...,  0,  0,  0],\n",
       "       [20, 15, 26, ...,  0,  0,  0],\n",
       "       [25, 32, 16, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_sent_len = np.sum(train_features != 0 ,axis = 2)\n",
    "train_features_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39137,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_doc_len = np.sum(train_features_sent_len!=0 , axis = 1)\n",
    "train_features_doc_len.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = TensorDataset(torch.from_numpy(train_features) , torch.from_numpy(np.array(opt_label)) , torch.from_numpy(train_features_sent_len) , torch.from_numpy(train_features_doc_len))\n",
    "trainloader = DataLoader(traindataset , batch_size= 16 , shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Val \n",
    "document_val_reviews = [nltk.sent_tokenize(clean.clean_pipeline_phase1(text)) for text in val_reviews]\n",
    "sent_val_reviews = []\n",
    "for document in document_val_reviews:\n",
    "    tmp = []\n",
    "    for sent in document:\n",
    "        t = clean.clean_pipeline_phase2(sent)\n",
    "        t = t.lower()\n",
    "        if t != '':\n",
    "            tmp.append(t)\n",
    "    sent_val_reviews.append(tmp)\n",
    "sent_val_encoder = [[[word2int[w] for w in  sent.split() if w in word2int] for sent in review]  for review in sent_val_reviews]\n",
    "val_features = np.array([padding_sentence(review) for review in sent_val_encoder])\n",
    "val_features_sent_len = np.sum(val_features != 0 ,axis = 2)\n",
    "val_features_doc_len = np.sum(val_features_sent_len!=0 , axis = 1)\n",
    "valdataset = TensorDataset(torch.from_numpy(val_features) , torch.from_numpy(val_label) , torch.from_numpy(val_features_sent_len) , torch.from_numpy(val_features_doc_len))\n",
    "valloader = DataLoader(valdataset , batch_size = 32, shuffle =False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test \n",
    "document_test_reviews = [nltk.sent_tokenize(clean.clean_pipeline_phase1(text)) for text in test_reviews]\n",
    "sent_test_reviews = []\n",
    "words = []\n",
    "for document in document_test_reviews:\n",
    "    tmp = []\n",
    "    for sent in document:\n",
    "        t = clean.clean_pipeline_phase2(sent)\n",
    "        t = t.lower()\n",
    "        if t != '':\n",
    "            tmp.append(t)\n",
    "            words.extend(word_tokenize(t))\n",
    "    sent_test_reviews.append(tmp)\n",
    "sent_test_encoder = [[[word2int[w] for w in  sent.split() if w in word2int] for sent in review]  for review in sent_test_reviews]\n",
    "test_features = np.array([padding_sentence(review) for review in sent_test_encoder])\n",
    "test_features_sent_len = np.sum(test_features != 0 ,axis = 2)\n",
    "test_features_doc_len = np.sum(test_features_sent_len!=0 , axis = 1)\n",
    "testdataset = TensorDataset(torch.from_numpy(test_features) , torch.from_numpy(test_label) , torch.from_numpy(test_features_sent_len) , torch.from_numpy(test_features_doc_len))\n",
    "testloader = DataLoader(testdataset , batch_size = 32, shuffle =False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def masked_softmax(X ,valid_len):\n",
    "    if valid_len == None:\n",
    "      return nn.Softmax(dim = -1)(X)\n",
    "    maxlen = X.shape[-1]\n",
    "    repeatlen = X.shape[-2]\n",
    "    if valid_len.dim() == 1:\n",
    "        valid_len = valid_len.unsqueeze(1)\n",
    "    # print(valid_len.shape , valid_len.dim())\n",
    "    mask = torch.arange(maxlen)[None, :] < valid_len[:,:,None]\n",
    "    mask = mask.repeat(1,repeatlen,1)\n",
    "    assert mask.shape == X.shape\n",
    "    X[~mask] = 1e-32\n",
    "    # print('X after masking contains nan:' , torch.isnan(X).any() )\n",
    "    return nn.Softmax(dim = -1)(X)\n",
    "\n",
    "class MLPAttention(nn.Module):\n",
    "    def __init__(self, units, k_dim ,q_dim ,dropout):\n",
    "        super(MLPAttention , self).__init__()\n",
    "        self.W_k = nn.Linear(k_dim , units ,bias= False)\n",
    "        self.W_q = nn.Linear(q_dim , units ,bias= False)\n",
    "        self.v = nn.Linear(units , 1,bias = False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,query ,key ,value,valid_len):\n",
    "        query, key = self.W_q(query) , self.W_k(key)\n",
    "        # print('Query ',query.shape , key.shape , value.shape)\n",
    "        features = query.unsqueeze(dim=2) + key.unsqueeze(dim=1)\n",
    "        features = torch.tanh(features)\n",
    "        # print('features : ', features.shape)\n",
    "        scores = self.v(features).squeeze(dim=-1)\n",
    "        # print('Scores contain Nan ? : ', torch.isnan(scores).any())\n",
    "        attention_weights = self.dropout(masked_softmax(scores, valid_len))\n",
    "        # print('Attn_weigths contain Nan ? : ', torch.isnan(attention_weights).any())\n",
    "        return torch.bmm(attention_weights , value)\n",
    "        \n",
    "class WordEncoder(nn.Module):\n",
    "    def __init__(self ,embeddings, vocab_size , emb_size, hidden_size , num_layers = 1 ,pad_id = 0):\n",
    "        super(WordEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings , freeze = False ,padding_idx= pad_id )\n",
    "        self.GRU = nn.GRU(input_size = self.emb_size , hidden_size = self.hidden_size , num_layers = self.num_layers , bidirectional = True , batch_first = True)\n",
    "        # self.linear_relu1 = nn.Sequential(\n",
    "        #     nn.Linear(2 * self.hidden_size , self.hidden_size*2),\n",
    "        #     nn.Dropout(0.4),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        self.linear_relu2 = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size , self.hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0 = self.initialize_weights(x.shape[0])\n",
    "        embs = self.embedding(x).float()\n",
    "        output , hn = self.GRU(embs)\n",
    "        # output = self.linear_relu1(output)\n",
    "        return self.linear_relu2(output) , hn\n",
    "    \n",
    "    def initialize_weights(self, batch_size):\n",
    "        h0 = torch.rand(size = (self.num_layers * 2 ,batch_size, self.hidden_size))\n",
    "        return h0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeirachicalAttentionModel(nn.Module):\n",
    "    def __init__(self ,embeddings , word2int):\n",
    "        super(HeirachicalAttentionModel , self).__init__()\n",
    "        self.wordEncoder = WordEncoder(embeddings,vocab_size  = len(word2int) , emb_size= 300 , hidden_size= 300)\n",
    "        self.attn_sent = MLPAttention(units = 300 , k_dim = 300 , q_dim= 300, dropout = 0.1)\n",
    "        self.attn_doc = MLPAttention(units = 300 , k_dim = 300 , q_dim= 300, dropout = 0.1)\n",
    "        self.GRU = nn.GRU(input_size = 300 , hidden_size = 300 , bidirectional = True , batch_first = True)\n",
    "        self.linear_relu1 = nn.Sequential(\n",
    "            nn.Linear(300,150),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.linear_relu2 = nn.Sequential(\n",
    "        #     nn.Linear(150,100),\n",
    "        #     nn.Dropout(0.4),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        self.linear_softmax = nn.Sequential(\n",
    "            nn.Linear(150 , 2 ),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X , X_sent_len , X_doc_len):\n",
    "        sent_reps = []\n",
    "        for i in range(len(X)):\n",
    "            doc = X[i]\n",
    "            output , hn = self.wordEncoder(doc)\n",
    "\n",
    "            context = self.attn_sent(output , output, output , X_sent_len[i])\n",
    "            sent_rep = torch.mean(context, dim =1)\n",
    "            # print(torch.isnan(sent_rep).any())\n",
    "            sent_reps.append(sent_rep.unsqueeze(dim =0))\n",
    "\n",
    "        sent_reps = torch.concat(sent_reps , dim = 0)\n",
    "        sent_rep, hn = self.GRU(sent_rep)\n",
    "        context = self.attn_doc(sent_reps , sent_reps, sent_reps , X_doc_len)\n",
    "        doc_reps = torch.mean(context, dim =1)\n",
    "        x1 = self.linear_relu1(doc_reps)\n",
    "        # x2 = self.linear_relu2(x1)\n",
    "        out = self.linear_softmax(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "model = HeirachicalAttentionModel(torch.from_numpy(embeddings),word2int).to(device)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters() , lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader ,valloader ,testloader, model, criteria , optimizer ,epochs ,device):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        total  = 0\n",
    "        acc = 0\n",
    "        model = model.to(device)\n",
    "        for input, label , input_sent_len , input_doc_len in tqdm(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            input , label = input.to(device) , label.to(device)\n",
    "            output = model(input, input_sent_len ,input_doc_len)\n",
    "            onehotenc_label = nn.functional.one_hot(label,num_classes = 2).float()\n",
    "            loss = criteria(output, onehotenc_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            res = torch.argmax(output , dim = 1)\n",
    "            acc +=torch.sum(res == label)\n",
    "            total+=len(label)\n",
    "            del input , label ,output\n",
    "\n",
    "        val_acc = 0\n",
    "        val_total = 0\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for input, label , input_sent_len , input_doc_len in tqdm(valloader):\n",
    "                input , label = input.to(device) , label.to(device)\n",
    "                output = model(input, input_sent_len ,input_doc_len)\n",
    "                onehotenc_label = nn.functional.one_hot(label,num_classes = 2).float()\n",
    "                loss = criteria(output, onehotenc_label)\n",
    "                train_loss.append(loss.item())\n",
    "                res = torch.argmax(output , dim = 1)\n",
    "                val_acc +=torch.sum(res == label)\n",
    "                val_total+=len(label)\n",
    "                val_loss.append(loss.item())\n",
    "                del input, label , output\n",
    "            test_acc = 0\n",
    "            test_total = 0\n",
    "            test_loss = []\n",
    "            for input, label , input_sent_len , input_doc_len in tqdm(testloader):\n",
    "                input , label = input.to(device) , label.to(device)\n",
    "                output = model(input, input_sent_len ,input_doc_len)\n",
    "                onehotenc_label = nn.functional.one_hot(label,num_classes = 2).float()\n",
    "                loss = criteria(output, onehotenc_label)\n",
    "                train_loss.append(loss.item())\n",
    "                res = torch.argmax(output , dim = 1)\n",
    "                test_acc +=torch.sum(res == label)\n",
    "                test_total+=len(label)\n",
    "                test_loss.append(loss.item())\n",
    "                del input,label , output\n",
    "            model = model.to('cpu')\n",
    "        print(f'Epoch {epoch} || train loss : {np.mean(train_loss)} || train accuracy : {acc/total} ||\\\n",
    "             val loss : {np.mean(val_loss)} || val accuracy : {val_acc / val_total} ||\\\n",
    "             test loss : {np.mean(test_loss)} || test accuracy : {test_acc / test_total}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:46<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.91it/s]\n",
      "100%|██████████| 313/313 [00:35<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 || train loss : 0.5244646837950993 || train accuracy : 0.7490609884262085 ||             val loss : 0.449000453710556 || val accuracy : 0.8575000166893005 ||             test loss : 0.4583216201002225 || test accuracy : 0.8471999764442444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:46<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:13<00:00,  8.99it/s]\n",
      "100%|██████████| 313/313 [00:34<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 || train loss : 0.42761961206199806 || train accuracy : 0.8819786310195923 ||             val loss : 0.42512209486961366 || val accuracy : 0.8817500472068787 ||             test loss : 0.43446030508215056 || test accuracy : 0.8727999925613403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:44<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.66it/s]\n",
      "100%|██████████| 313/313 [00:35<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 || train loss : 0.40233318067092827 || train accuracy : 0.9122568964958191 ||             val loss : 0.4189491131305695 || val accuracy : 0.8892500400543213 ||             test loss : 0.4292448147798118 || test accuracy : 0.8805999755859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:46<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.39it/s]\n",
      "100%|██████████| 313/313 [00:37<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 || train loss : 0.38715863989172195 || train accuracy : 0.9320591688156128 ||             val loss : 0.4251722528934479 || val accuracy : 0.8837500214576721 ||             test loss : 0.4316097369399695 || test accuracy : 0.8758999705314636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:46<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.90it/s]\n",
      "100%|██████████| 313/313 [00:34<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 || train loss : 0.3758372200836962 || train accuracy : 0.9446303844451904 ||             val loss : 0.4168737893104553 || val accuracy : 0.8910000324249268 ||             test loss : 0.4237282876960767 || test accuracy : 0.8858000040054321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:45<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.69it/s]\n",
      "100%|██████████| 313/313 [00:36<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 || train loss : 0.36959890457745026 || train accuracy : 0.9535988569259644 ||             val loss : 0.4262175648212433 || val accuracy : 0.8825000524520874 ||             test loss : 0.4313096538328896 || test accuracy : 0.8766999840736389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:45<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:16<00:00,  7.37it/s]\n",
      "100%|██████████| 313/313 [00:42<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 || train loss : 0.36300183417892123 || train accuracy : 0.9601910710334778 ||             val loss : 0.41680330300331114 || val accuracy : 0.893250048160553 ||             test loss : 0.42313673873298085 || test accuracy : 0.8862999677658081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:44<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:18<00:00,  6.73it/s]\n",
      "100%|██████████| 313/313 [00:41<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 || train loss : 0.3602052236140497 || train accuracy : 0.9651480317115784 ||             val loss : 0.4247013831138611 || val accuracy : 0.8852500319480896 ||             test loss : 0.43232934924360283 || test accuracy : 0.8762999773025513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:45<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:16<00:00,  7.40it/s]\n",
      "100%|██████████| 313/313 [00:41<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 || train loss : 0.35509590763773 || train accuracy : 0.9695428609848022 ||             val loss : 0.417304105758667 || val accuracy : 0.893000066280365 ||             test loss : 0.4208654283334653 || test accuracy : 0.8898999691009521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:47<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.63it/s]\n",
      "100%|██████████| 313/313 [00:36<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 || train loss : 0.35410055225808656 || train accuracy : 0.9709993004798889 ||             val loss : 0.41730486011505125 || val accuracy : 0.893000066280365 ||             test loss : 0.4214113828854058 || test accuracy : 0.8888999819755554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:46<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:16<00:00,  7.43it/s]\n",
      "100%|██████████| 313/313 [00:38<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 || train loss : 0.3517294024883978 || train accuracy : 0.9735032916069031 ||             val loss : 0.4199540374279022 || val accuracy : 0.890250027179718 ||             test loss : 0.42055011919131297 || test accuracy : 0.8894000053405762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:46<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:15<00:00,  8.19it/s]\n",
      "100%|██████████| 313/313 [00:35<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 || train loss : 0.35098390886027436 || train accuracy : 0.9749086499214172 ||             val loss : 0.4182258548736572 || val accuracy : 0.8927500247955322 ||             test loss : 0.4230386384378988 || test accuracy : 0.8877999782562256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:44<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:14<00:00,  8.34it/s]\n",
      "100%|██████████| 313/313 [00:40<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 || train loss : 0.3506578174372893 || train accuracy : 0.9760839939117432 ||             val loss : 0.42071370482444764 || val accuracy : 0.890250027179718 ||             test loss : 0.42745800216357926 || test accuracy : 0.8833999633789062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:41<00:00,  2.31it/s]\n",
      "100%|██████████| 125/125 [00:16<00:00,  7.36it/s]\n",
      "100%|██████████| 313/313 [00:43<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 || train loss : 0.3486521302104203 || train accuracy : 0.9775915145874023 ||             val loss : 0.421666695356369 || val accuracy : 0.8892500400543213 ||             test loss : 0.4218331399245765 || test accuracy : 0.8883999586105347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:48<00:00,  2.29it/s]\n",
      "100%|██████████| 125/125 [00:16<00:00,  7.48it/s]\n",
      "100%|██████████| 313/313 [00:42<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 || train loss : 0.34825968028561083 || train accuracy : 0.9780769944190979 ||             val loss : 0.41982651567459106 || val accuracy : 0.8907500505447388 ||             test loss : 0.42448814122821577 || test accuracy : 0.885699987411499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2447/2447 [17:44<00:00,  2.30it/s]\n",
      "100%|██████████| 125/125 [00:16<00:00,  7.72it/s]\n",
      "100%|██████████| 313/313 [00:40<00:00,  7.71it/s]\n"
     ]
    }
   ],
   "source": [
    "train(trainloader ,valloader , testloader,model,  criteria, optimizer , 20 , device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f037b47d1022de7de502d0c0924ab44d4c207b414c797253101d6506b327664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
